786!!!

Webscrapping:

	-Three main libraries for doing web scrapping as below:
	
		-beautiful soup
		
		-selinium 
		
		-scrapyy
	-but which one is the best
	
	-BeautifulSoup
	
		-html/xml files
		
		-best for beginners
		
		-easiest 
		
		-few lines of code to scrap a site
		
		-easy to setup
		
		-just install library
		
		-no support for javascript sites
		
		-inefficient
		
		-it has some dependencies
		
	-Selinum:
	
		-web driver
		
		-used for web scrapping
		
		-seliunium is one of the best library
		
		-easier to learn as comp to scrapy
		
		-its slower 
		
		-webscrapping with it is slower
		
	-Scrapy:
	
		-web scrapping framework
		
		-written in python
		
		-most complete webscrapping tool
		
		-harder to learn
		
		-it is fast 
		
		-it is efficient
		
		-scrapy is the most complete framework in python
		
		-databases are used in scrapy
		
	-Beautfiful soup for beginner
	
	-selinium for small projects
	
	-scrapy for large projects
	
Video 2:

	-We are going to used pycharm
	
	-download and install python as well 
	
	-downlaod python from their site
	
	-once downloaded then click on it and install the python
	
	-to verify python installed 
	
	-open terminal/cmd and write python --version
	
	-if it shows the version of the python it means that the python is installed on your pc
	
	-if nothing is displayed, it means that the python is not installed 
	
	-We are going to use the pycharm ide in this course
	
	-but i'm going to use the juypter-notebook or visual studio code for myself 
	
Video 3:

	-Basic for python for webscrapping
	
	-common for all the frameworks
	
	-u know how to print basic hello world in python
	
	-python list to store multiple things in single variable
	
	-states=["California","Texas","Florida","NewYork"]
	
	-These are the four states in a square bracket its a list of name states
	
	-It is a list of string
	
	-List has index
	
	-Python use 0 base index
	
	-Claifronia has index 0
	
	-and so on
	
	-used square bracket to access index
	
	-states[0]
	
	-it will display california
	
	-print(states[0])
	
	-california will be displayed
	
	-print(states[3])
	
	-New York will be displayed
	
	-we can also use minus indexes from the end of the list
	
	-print(states[-1])
	
	-it will display New York
	
	-New York is on -1
	
	-print(states[-4])
	
	-california will be displa
	
	-indexing and lists are very important
	
	-Loops:
	
		-for keyword is used
		
		-for state in states:
		
			print(state)
			
		-this is the format of a for loop
		
		-Lets run this code 
		
		-we will get the four states
		
	-If statements:
	
		-if true body will run if not will not run
		
		-if state=="Florida":
		
			print(state)
			
		-put this block in for loop
		
		-for state in states:
			if state=="Florida":
				print(state)
				
		-now in this code if the state will be florida, it will be printed else nothing will be displayed
		
		-florida will be displayed
		
		-we are looping through all the list but only florida will be printed
		
	-Element
	
	-We will look through the elements
	
	-We wil get the text that we want
	
	-Getting multiple elements in list
	
	-looping list
	
	-get the important info from it
	
	-this is called web scrapping
	
	-How to export data that we scrape?
	
		-how to export data in python
		
		-simplest way is:
		
			-with open
			
			-with open ('test.txt','w') as file:
				file.write("Data Successfully scraped!")
			
			-w will delete the existing file content with the new one
			
			-file.write() we are writing something in the file
			
			-test.txt,'w' as file, means test.txt,'w' become file:
			
			-The file will be exported in our local directory 
			
		-Second way to export data:
		
			-We can do this with a third party as well
			
			-using pandas library
			
			-use pip install pandas
			
			-pip install panadas
			
			-it will install the panads library in the pc
			
			-To use this library, import it,
			
				-import pandas as pd
				
				-//by this we will use pd instead of kieyword pandas
				
				-states=["california","Texas","Florida","New York"]
				
				-population=[393334,2342334,224322,22323]
				
				-//new we are going to create a dictionary 
				
				-//To create a dictionary use this method below one,
				
				-dict_states={'States':states,'Population':population}
				
				-//dictionary has a key and a value
				
				-//keys are unique
				
				-//dictionary also help us better manage the list in python
				
				-//To create a dataframe use,
				
				-df_states=pd.DataFrame.from_dict(dict_states)
				
				-//we are creating a dataframe from a dictionary
				
				-print(df_states)
				
				-//now observer the output
				
				-//this is the format of excel data sheet
				
				-//export it into csv
				
				-df_states.to_csv('states.csv')
				
				-we will now have a file states.csv file in our directory containing all the data
				
				-Dataframes work with the index
				
				-df_states.to_csv('states.csv',index='False')
				
				-After this we'll not have any index in the csv file 
				
				-This was all about working with the data
				
	-How to handle exceptions in data?
	
		-use try and except
		
		-new_list=[2,4,6,'California']
		
		-list with 3 number and 1 string
		
		-we have a mixture
		
		-let's observe an error
		
			-for element in new_list:
				print(element/2)
				
			-it will give error when our loop will reach at string
			
			-python will show us an error
			
			-in the forth iteration
			
			-unsupported operand type
			
			-we can fix this
			
			-but while using data scrapping these errors can pass unseen
			
			-So we'll escape these errors with try
			
			-for element in new_list:
				try:
					print(element/2)
				except:
					print("The element is not a number")
					
				-so python will execute try normally
				
				-but when ever an error come, the except block will execute
				
				-run the code and obeserve
				
				-first 3 iterations will run but in forth one the exception will be thrown to avoid error which is "The element is not a number"
				
				-This is how try/except work
				
				-or this is how error handling work
				
				-this is to avoid the code to break just because of single error during web scrapping
				
				-it is used alot in webscrapping (try/except)
				
	While-Break:
	
		-simply its a while loop
		
		-n=4
		while n>0:
			print(n)
			n=n-1
		print('Message')
		
		-let's test this while loop, the loop will run until the value of n will become 0
		
		-when the loop statement will become false the while loop will break
		
		-Another wayy to break while loop is break statement
		
		-n=4
		while n>0:
			print(n)
			n=n-1
			if n==2:
				break
		print('Loop ended')
		
	-This was all the basics of python for the webscrapping course
	
Video 6: 

	-HTML basic
	
	-when scrapping a website
	
	-html=hyper text markup language
	
	-basic tool of a website
	
	-structure of a web content
	
	-get familiar with html basics
	
	-understand code behind site
	
	-best way to scrap site
	
	-HTML Markup Syntax
	
	-Tag Name=h1
	
	-Most tags has opening and closing tags
	
	-h1 and /h1
	
	-some tags don't need closing tag
	
	-second tag is tag attribute
	
	-found inside the tag
	
	-attribute has values
	
	-Affected content
	
	-it is the text that we see on website
	
	-it is affected by tags and attribute defined
	
	-This all collectiveley called as html element
	
	-Tags:
	
		-head tag <head> for metadata
		
		-body <body> body
		
		-<header> for layout
		
		-<article>
		
		-//inspect button on browser is used to inspect html code of a site
		
		-<header>
		
		-<article> for books e.t.c
		
		-<p> for paragraph
		
		-<h1>,<h2>,<h3> are level 1,level2 and level3 headings
		
		-following tags will be commonly present
		
			-<div> for container
			
			-<nav> padding
			
			-<li> for list item
			
			-<a> for anchor or hyperlink or link
			
				-<a href> is hyper link
			
			-<button tag>
			
			-<table> for table
			
			-<td> for table data
			
			-<tr> for table row
			
			-<ul> onordered list tag
			
			-<iframe> or nested browsing
			
Video 7: 

	-html code
	
	-html basics
	
	-choose simple web
	
	-title 
	
	-plot
	
	-transcript
	
	-these area 3 compon of web
	
	-we are looking at title Titanic
	
	-tax are words surrounding by brackets
	
	-e.g article,h1,div are tax
	
	-tree struture for effective web scrap
	
	-structure of tree:
	
		-article(root)
		
		-it has h1 tag
		
		-it has attribute element
		
		-it has element tag
		
		-last is the div element 
		
		-all of these are child nodes of article
		
		-attributes don't have any child
		
		-tags can have childern
		
		-We will first find:
		
			-id
			
			-class name
			
			-tag name
			
			-Xpath
			
		-We will find these thing in order when doing web scrapping
		
		-Xpath is btw not available in beautiful soup
		
		
